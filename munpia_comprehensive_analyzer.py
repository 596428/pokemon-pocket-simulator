# -*- coding: utf-8 -*-
# Munpia Individual Novel ID Finder - ì‹¤ì œ ì‘í’ˆ ID ì°¾ê¸°

import requests
from bs4 import BeautifulSoup
import json
import re
import time
import random

class MunpiaNovelIdFinder:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Connection': 'keep-alive',
            'Referer': 'https://www.munpia.com/'
        })
        
        try:
            self.session.get("https://www.munpia.com/", timeout=10)
            time.sleep(1)
        except:
            pass
    
    def extract_novel_ids_from_html(self, html_content):
        """HTMLì—ì„œ ì‘í’ˆ ID ì¶”ì¶œ"""
        
        novel_ids = []
        
        # íŒ¨í„´ 1: novel.munpia.com/ìˆ«ì
        pattern1 = re.findall(r'novel\.munpia\.com/(\d+)', html_content)
        novel_ids.extend(pattern1)
        
        # íŒ¨í„´ 2: /novel/ìˆ«ì
        pattern2 = re.findall(r'/novel/(\d+)', html_content)
        novel_ids.extend(pattern2)
        
        # íŒ¨í„´ 3: novelId=ìˆ«ì
        pattern3 = re.findall(r'novelId=(\d+)', html_content)
        novel_ids.extend(pattern3)
        
        # íŒ¨í„´ 4: JavaScriptë‚˜ AJAX í˜¸ì¶œì—ì„œ
        pattern4 = re.findall(r'id["\']?\s*[:=]\s*["\']?(\d{4,7})["\']?', html_content)
        novel_ids.extend(pattern4)
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
        unique_ids = list(set(novel_ids))
        
        # ìˆ«ìê°€ ë„ˆë¬´ í¬ê±°ë‚˜ ì‘ìœ¼ë©´ ì œì™¸ (ì¼ë°˜ì ìœ¼ë¡œ ì‘í’ˆ IDëŠ” 4-7ìë¦¬)
        valid_ids = [id for id in unique_ids if 1000 <= int(id) <= 9999999]
        
        return valid_ids
    
    def find_novel_ids_from_multiple_pages(self):
        """ì—¬ëŸ¬ í˜ì´ì§€ì—ì„œ ì‘í’ˆ ID ìˆ˜ì§‘"""
        
        print("ë¬¸í”¼ì•„ ê°œë³„ ì‘í’ˆ ID ìˆ˜ì§‘")
        print("=" * 50)
        
        # ë‹¤ì–‘í•œ í˜ì´ì§€ì—ì„œ ID ìˆ˜ì§‘
        pages_to_check = [
            "https://www.munpia.com/ranking/novel",
            "https://www.munpia.com/",
            "https://www.munpia.com/page/j/view/w/best/today",
            "https://www.munpia.com/page/j/view/w/best/week",
            "https://www.munpia.com/page/j/view/w/genre/13",  # íŒíƒ€ì§€
            "https://www.munpia.com/page/j/view/w/genre/21",  # ë¬´í˜‘
            "https://www.munpia.com/page/j/view/w/genre/23"   # ë¡œë§¨ìŠ¤
        ]
        
        all_novel_ids = []
        
        for page_url in pages_to_check:
            print(f"\ní˜ì´ì§€ ë¶„ì„: {page_url}")
            
            try:
                time.sleep(random.uniform(1, 3))
                response = self.session.get(page_url, timeout=15)
                
                if response.status_code == 200:
                    novel_ids = self.extract_novel_ids_from_html(response.text)
                    print(f"  ë°œê²¬ëœ ID: {len(novel_ids)}ê°œ")
                    all_novel_ids.extend(novel_ids)
                    
                    if novel_ids:
                        print(f"  ìƒ˜í”Œ: {novel_ids[:5]}")
                else:
                    print(f"  HTTP {response.status_code}")
                    
            except Exception as e:
                print(f"  ì˜¤ë¥˜: {e}")
        
        # ì¤‘ë³µ ì œê±°
        unique_novel_ids = list(set(all_novel_ids))
        print(f"\nì´ ê³ ìœ  ì‘í’ˆ ID: {len(unique_novel_ids)}ê°œ")
        
        return unique_novel_ids[:50]  # ìƒìœ„ 50ê°œë§Œ ë°˜í™˜
    
    def analyze_novel_with_id(self, novel_id):
        """ì‘í’ˆ IDë¡œ ê°œë³„ ì‘í’ˆ ë¶„ì„"""
        
        novel_url = f"https://novel.munpia.com/{novel_id}"
        print(f"\n[ID {novel_id}] ë¶„ì„: {novel_url}")
        
        try:
            time.sleep(random.uniform(2, 4))
            response = self.session.get(novel_url, timeout=15)
            
            if response.status_code != 200:
                print(f"  ì ‘ê·¼ ì‹¤íŒ¨: HTTP {response.status_code}")
                return None
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            novel_data = {
                'novel_id': novel_id,
                'url': novel_url,
                'title': None,
                'author': None,
                'genre': None,
                'views': None,
                'rating': None,
                'synopsis': None,
                'upload_date': None,
                'episode_count': None,
                'status': None,
                'tags': [],
                'favorites': None
            }
            
            # ì œëª© ì°¾ê¸° (ê°œì„ ëœ ë°©ë²•)
            title_selectors = [
                'h1.title', '.novel-title', '.work-title', 
                'h1', 'title', '.subject', '.name'
            ]
            
            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem:
                    title = title_elem.get_text().strip()
                    if title and len(title) > 2 and 'ë¬¸í”¼ì•„' not in title and 'ì—ëŸ¬' not in title:
                        novel_data['title'] = title
                        print(f"  ì œëª©: {title}")
                        break
            
            # ë§Œì•½ ì œëª©ì„ ì°¾ì§€ ëª»í–ˆë‹¤ë©´ í˜ì´ì§€ íƒ€ì´í‹€ì—ì„œ
            if not novel_data['title']:
                page_title = soup.find('title')
                if page_title:
                    title = page_title.get_text().strip()
                    if 'ë¬¸í”¼ì•„' in title:
                        title = title.replace('- ì›¹ì†Œì„¤ ë¬¸í”¼ì•„', '').strip()
                    if len(title) > 2:
                        novel_data['title'] = title
                        print(f"  ì œëª©(íƒ€ì´í‹€): {title}")
            
            # ì „ì²´ í˜ì´ì§€ í…ìŠ¤íŠ¸ì—ì„œ ì •ë³´ ì¶”ì¶œ
            page_text = soup.get_text()
            
            # ì‘ê°€ ì°¾ê¸°
            author_patterns = [
                r'ì‘ê°€\s*[:ï¼š]\s*([ê°€-í£a-zA-Z0-9_\s]{2,20})',
                r'ê¸€\s*[:ï¼š]\s*([ê°€-í£a-zA-Z0-9_\s]{2,20})',
                r'ì €ì\s*[:ï¼š]\s*([ê°€-í£a-zA-Z0-9_\s]{2,20})'
            ]
            
            for pattern in author_patterns:
                match = re.search(pattern, page_text)
                if match:
                    author = match.group(1).strip()
                    if len(author) > 1 and len(author) < 20:
                        novel_data['author'] = author
                        print(f"  ì‘ê°€: {author}")
                        break
            
            # ì¥ë¥´ ì°¾ê¸°
            genre_keywords = ['íŒíƒ€ì§€', 'ë¬´í˜‘', 'ë¡œë§¨ìŠ¤', 'BL', 'GL', 'í˜„ëŒ€', 'ì—­ì‚¬', 'ì¶”ë¦¬', 'SF', 'ì˜¤ì»¬íŠ¸']
            for genre in genre_keywords:
                if genre in page_text:
                    novel_data['genre'] = genre
                    print(f"  ì¥ë¥´: {genre}")
                    break
            
            # ì¡°íšŒìˆ˜ ì°¾ê¸° (ê°œì„ ëœ íŒ¨í„´)
            view_patterns = [
                r'ì¡°íšŒìˆ˜?\s*[:ï¼š]?\s*([\d,]+(?:ë§Œ|ì²œ)?)',
                r'ì¡°íšŒ\s*([\d,]+(?:ë§Œ|ì²œ)?)',
                r'view\s*[:ï¼š]?\s*([\d,]+(?:ë§Œ|ì²œ)?)',
                r'([\d,]+(?:ë§Œ|ì²œ)?)\s*íšŒ\s*ì¡°íšŒ',
                r'Total\s*View\s*[:ï¼š]?\s*([\d,]+)',
                r'ì´\s*ì¡°íšŒ\s*[:ï¼š]?\s*([\d,]+)'
            ]
            
            for pattern in view_patterns:
                match = re.search(pattern, page_text, re.IGNORECASE)
                if match:
                    views = match.group(1)
                    novel_data['views'] = views
                    print(f"  ì¡°íšŒìˆ˜: {views}")
                    break
            
            # ë³„ì  ì°¾ê¸°
            rating_patterns = [
                r'ë³„ì \s*[:ï¼š]?\s*(\d+\.?\d*)',
                r'í‰ì \s*[:ï¼š]?\s*(\d+\.?\d*)',
                r'â˜…\s*(\d+\.?\d*)',
                r'rating\s*[:ï¼š]?\s*(\d+\.?\d*)',
                r'(\d+\.?\d*)\s*/\s*(?:10|5)',
                r'(\d+\.?\d*)\s*ì '
            ]
            
            for pattern in rating_patterns:
                match = re.search(pattern, page_text, re.IGNORECASE)
                if match:
                    rating = float(match.group(1))
                    if 0 <= rating <= 10:
                        novel_data['rating'] = match.group(1)
                        print(f"  ë³„ì : {match.group(1)}")
                        break
            
            # ì‹œë†‰ì‹œìŠ¤ ì°¾ê¸° (ê°œì„ ëœ ë°©ë²•)
            synopsis_selectors = [
                '.synopsis', '.summary', '.description', '.intro',
                '.content', '.story', '.plot', '[class*="synopsis"]'
            ]
            
            for selector in synopsis_selectors:
                synopsis_elem = soup.select_one(selector)
                if synopsis_elem:
                    synopsis = synopsis_elem.get_text().strip()
                    if synopsis and len(synopsis) > 50:
                        novel_data['synopsis'] = synopsis[:300]
                        print(f"  ì‹œë†‰ì‹œìŠ¤: {synopsis[:50]}...")
                        break
            
            # ë©”íƒ€ íƒœê·¸ì—ì„œ ì‹œë†‰ì‹œìŠ¤
            if not novel_data['synopsis']:
                meta_desc = soup.find('meta', {'name': 'description'})
                if meta_desc:
                    content = meta_desc.get('content', '').strip()
                    if len(content) > 30 and 'ë¬¸í”¼ì•„' not in content:
                        novel_data['synopsis'] = content
                        print(f"  ë©”íƒ€ ì‹œë†‰ì‹œìŠ¤: {content[:50]}...")
            
            # ì—…ë¡œë“œ ë‚ ì§œ
            date_patterns = [
                r'ì—…ë°ì´íŠ¸\s*[:ï¼š]?\s*(\d{4}[-./]\d{1,2}[-./]\d{1,2})',
                r'ë“±ë¡\s*[:ï¼š]?\s*(\d{4}[-./]\d{1,2}[-./]\d{1,2})',
                r'(\d{4}\.\d{1,2}\.\d{1,2})',
                r'(\d{4}-\d{1,2}-\d{1,2})'
            ]
            
            for pattern in date_patterns:
                match = re.search(pattern, page_text)
                if match:
                    date = match.group(1)
                    novel_data['upload_date'] = date
                    print(f"  ì—…ë¡œë“œ: {date}")
                    break
            
            # íšŒì°¨/ì—í”¼ì†Œë“œ ìˆ˜
            episode_patterns = [
                r'ì´\s*(\d+)\s*í™”',
                r'ì´\s*(\d+)\s*íšŒ',
                r'(\d+)\s*í™”\s*ì™„ê²°',
                r'ì „ì²´\s*(\d+)\s*í™”'
            ]
            
            for pattern in episode_patterns:
                match = re.search(pattern, page_text)
                if match:
                    episodes = match.group(1)
                    novel_data['episode_count'] = episodes
                    print(f"  íšŒì°¨ìˆ˜: {episodes}í™”")
                    break
            
            # ì¢‹ì•„ìš”/ì°œí•˜ê¸° ìˆ˜
            favorite_patterns = [
                r'ì¢‹ì•„ìš”\s*[:ï¼š]?\s*([\d,]+)',
                r'ì°œ\s*[:ï¼š]?\s*([\d,]+)',
                r'ê´€ì‹¬\s*[:ï¼š]?\s*([\d,]+)'
            ]
            
            for pattern in favorite_patterns:
                match = re.search(pattern, page_text, re.IGNORECASE)
                if match:
                    favorites = match.group(1)
                    novel_data['favorites'] = favorites
                    print(f"  ì¢‹ì•„ìš”: {favorites}")
                    break
            
            # ì—°ì¬ ìƒíƒœ
            status_patterns = [
                r'(ì™„ê²°)', r'(ì—°ì¬ì¤‘)', r'(íœ´ì¬)', r'(ì¤‘ë‹¨)', r'(ì™„ë£Œ)'
            ]
            
            for pattern in status_patterns:
                match = re.search(pattern, page_text)
                if match:
                    status = match.group(1)
                    novel_data['status'] = status
                    print(f"  ìƒíƒœ: {status}")
                    break
            
            return novel_data
            
        except Exception as e:
            print(f"  ë¶„ì„ ì˜¤ë¥˜: {e}")
            return None
    
    def comprehensive_novel_analysis(self):
        """ì¢…í•©ì ì¸ ì‘í’ˆ ë¶„ì„"""
        
        print("ë¬¸í”¼ì•„ ê°œë³„ ì‘í’ˆ ì¢…í•© ë¶„ì„ v2.0")
        print("=" * 60)
        
        # 1. ì‘í’ˆ IDë“¤ ìˆ˜ì§‘
        novel_ids = self.find_novel_ids_from_multiple_pages()
        
        if not novel_ids:
            print("ë¶„ì„í•  ì‘í’ˆ IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        print(f"\nì´ {len(novel_ids)}ê°œ ì‘í’ˆ IDë¡œ ìƒì„¸ ë¶„ì„ ì‹œì‘")
        
        # 2. ê°œë³„ ì‘í’ˆë“¤ ë¶„ì„
        results = []
        successful_extractions = {
            'title': 0, 'author': 0, 'genre': 0, 'views': 0,
            'rating': 0, 'synopsis': 0, 'upload_date': 0,
            'episode_count': 0, 'status': 0, 'favorites': 0
        }
        
        for i, novel_id in enumerate(novel_ids[:15]):  # ì²˜ìŒ 15ê°œë§Œ í…ŒìŠ¤íŠ¸
            print(f"\n--- ì‘í’ˆ {i+1}/{min(15, len(novel_ids))} ---")
            
            novel_data = self.analyze_novel_with_id(novel_id)
            if novel_data:
                results.append(novel_data)
                
                # ì„±ê³µë¥  ê³„ì‚°
                for key in successful_extractions:
                    if novel_data.get(key):
                        successful_extractions[key] += 1
        
        # 3. ê²°ê³¼ ë¶„ì„ ë° ì €ì¥
        total_analyzed = len(results)
        if total_analyzed > 0:
            print(f"\n=== ìµœì¢… ì¢…í•© ê²°ê³¼ ===")
            print(f"ì„±ê³µì ìœ¼ë¡œ ë¶„ì„ëœ ì‘í’ˆ: {total_analyzed}ê°œ")
            
            print(f"\në°ì´í„°ë³„ ì¶”ì¶œ ì„±ê³µë¥ :")
            korean_names = {
                'title': 'ì œëª©', 'author': 'ì‘ê°€', 'genre': 'ì¥ë¥´', 
                'views': 'ì¡°íšŒìˆ˜', 'rating': 'ë³„ì ', 'synopsis': 'ì‹œë†‰ì‹œìŠ¤',
                'upload_date': 'ì—…ë¡œë“œë‚ ì§œ', 'episode_count': 'íšŒì°¨ìˆ˜', 
                'status': 'ì—°ì¬ìƒíƒœ', 'favorites': 'ì¢‹ì•„ìš”ìˆ˜'
            }
            
            for key, count in successful_extractions.items():
                success_rate = (count / total_analyzed) * 100
                status = "SUCCESS" if success_rate >= 70 else "PARTIAL" if success_rate >= 30 else "FAILED"
                korean_name = korean_names.get(key, key)
                print(f"{status:8} {korean_name:10}: {success_rate:5.1f}% ({count:2}/{total_analyzed:2})")
        
        # ê²°ê³¼ ì €ì¥
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f'munpia_comprehensive_analysis_{timestamp}.json'
        
        final_results = {
            'timestamp': timestamp,
            'total_novel_ids_found': len(novel_ids),
            'analyzed_count': total_analyzed,
            'success_rates': {key: (count/total_analyzed*100) if total_analyzed > 0 else 0 
                            for key, count in successful_extractions.items()},
            'novels': results
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(final_results, f, ensure_ascii=False, indent=2)
        
        print(f"\nìƒì„¸ ê²°ê³¼ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return final_results

def main():
    print("ë¬¸í”¼ì•„ ê°œë³„ ì‘í’ˆ ID ê¸°ë°˜ ì¢…í•© ë¶„ì„")
    print("ëª©í‘œ: ì‹¤ì œ ì‘í’ˆ í˜ì´ì§€ì—ì„œ ëª¨ë“  ìƒì„¸ ì •ë³´ ì¶”ì¶œ")
    print("=" * 80)
    
    finder = MunpiaNovelIdFinder()
    results = finder.comprehensive_novel_analysis()
    
    if results:
        avg_success = sum(results['success_rates'].values()) / len(results['success_rates'])
        print(f"\nì „ì²´ í‰ê·  ì¶”ì¶œ ì„±ê³µë¥ : {avg_success:.1f}%")
        
        if avg_success >= 60:
            print("ğŸ‰ ë¬¸í”¼ì•„ ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì„±ê³µ! ë„¤ì´ë²„ì™€ ê²½ìŸ ê°€ëŠ¥í•œ ìˆ˜ì¤€ì…ë‹ˆë‹¤.")
        elif avg_success >= 40:
            print("ğŸŸ¡ ë¶€ë¶„ì  ì„±ê³µ. ëŒ€ë¶€ë¶„ì˜ ì •ë³´ëŠ” ì¶”ì¶œ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        else:
            print("ğŸ”´ ì¶”ê°€ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
